- hosts: "{{ env }}"
  # Currently requires a vault file even if empty
  vars_files:
    - "{{ config_dir }}/config.common.yml"
    - "{{ config_dir }}/config.{{ env }}.yml"
    - "{{ vault }}"
  gather_facts: true
  become: yes
  environment:
    PATH: "{{ ansible_env.PATH }}:/home/{{ application_user }}/.pyenv/shims"
  

  # For whatever reason, for new deployments, it seems like nginx is not
  # getting restarted at the right time. Thus for new deploys only, it is
  # necessary to manually kick nginx.
  #
  # ^ this may no longer be true. Some changes were made b/c nginx was not
  # restarting in general and might have fixed this for new deploys.
  # See further notes in deploy.web.yml

  tasks:

    - name: Template .env environment file
      template:
        src: templates/env.j2
        dest: "{{ env_file }}"
      notify:
        - restart appserver

    - name: Template envrc startup script
      template:
        src: templates/envrc.j2
        dest: "{{ envrc_file }}"
      notify:
        - allow envrc
        - restart appserver

    - name: Template systemd config
      template:
        src: templates/systemd.j2
        dest: "/etc/systemd/system/{{ service_name }}.service"
        owner: root
        group: root
        mode: 0644
      become: yes
      notify:
        - restart appserver
        - restart nginx

    - name: install python packages
      pip:
        requirements={{ requirements_file }}
        virtualenv={{ virtualenv }}
        virtualenv_command="{{ python }} -m venv --prompt {{ project_name }}"
      notify:
        - restart appserver
        - restart nginx

    - name: set project location in virtual environment
      template:
        src=templates/project.j2
        dest="{{ virtualenv }}/.project"

    - name: django migrate
      when: type == "django" and (django_migrate is undefined or django_migrate)
      # django manage.py command does not support env vars, thus shell instead
      shell: "{{ env_run_script }} ./manage.py migrate --noinput"
      args:
        chdir: "{{ application_dir }}"

    # service module is really flaky on custom upstart scripts. It seems more
    # consistent to use raw calls to service. This also requires sudoers:
    # apps    ALL=(ALL) NOPASSWD:ALL
    # Per various discussions, it seems like this is just how it is. See, e.g.:
    # https://github.com/ansible/ansible/issues/5712

    # Be sure service permissions are accessible by ansible user
    - name: make sure application server is running
      #  should we be using shell or command here instead of raw?
      raw: if (systemctl status {{ service_name }} |grep Stop); then systemctl start {{ service_name }} ; fi


  handlers:
    - name: restart appserver
      raw: systemctl restart {{ service_name }}
    - name: restart nginx
      shell: systemctl restart nginx
        warn=False
    - name: allow envrc
      shell: direnv allow .
      args:
        chdir: "{{ application_dir }}"
